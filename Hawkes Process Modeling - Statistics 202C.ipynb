{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Ritvik Kharkar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize, LinearConstraint\n",
    "from random import random\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lambda_at_t_val(params, lambda_0, F, T_vals, t):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "        params: ((k+1,)) parameters. First k are alpha then delta\n",
    "        lambda_0: the baseline lambda\n",
    "        F: (N x k) features\n",
    "        T_vals: (N x 1) event times\n",
    "        t: real number, current time stamp\n",
    "        lambda_0: the baseline event intensity\n",
    "    \n",
    "    OUTPUT:\n",
    "        value of the lambda function at t\n",
    "    \"\"\"\n",
    "    \n",
    "    #get alpha and delta\n",
    "    alpha = params[:-1].reshape(-1,1)\n",
    "    delta = params[-1]\n",
    "    \n",
    "    #create the event times mask\n",
    "    mask = (T_vals <= t)\n",
    "    \n",
    "    #get the T vals which are before the current time\n",
    "    T_vals_lim = T_vals[mask]\n",
    "    \n",
    "    #get the features which are before the current time\n",
    "    F_lim = F[mask]\n",
    "    \n",
    "    #get sum of weighted features\n",
    "    weights_lim = np.exp(delta * T_vals_lim)\n",
    "    sum_weighted_features_lim = (weights_lim * F_lim).sum(axis=0).reshape(-1,1)\n",
    "    \n",
    "    #get weighted alpha\n",
    "    weighted_alpha = (alpha * np.exp(-delta * t)).reshape(-1,1)\n",
    "    \n",
    "    #compute dot product\n",
    "    lambda_val = lambda_0 + (sum_weighted_features_lim * weighted_alpha).sum()\n",
    "        \n",
    "    #return final lambda value\n",
    "    return lambda_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lambda_at_T_vals(params, lambda_0, F, T_vals):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "        params: ((k+1,)) parameters. First k are alpha then delta\n",
    "        lambda_0: the baseline lambda\n",
    "        F: (N x k) features\n",
    "        T_vals: (N x 1) event times\n",
    "        lambda_0: real number, baseline event intensity\n",
    "    \n",
    "    OUTPUT:\n",
    "        value of the lambda function at each t in T_vals, (N x 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #get alpha and delta\n",
    "    alpha = params[:-1].reshape(-1,1)\n",
    "    delta = params[-1]\n",
    "    \n",
    "    #get the weighted scores\n",
    "    weights = np.exp(delta * T_vals)\n",
    "    weighted_scores = F * weights\n",
    "    \n",
    "    #get cumulative sum of weighted scores\n",
    "    cumul_sum_weighted_scores = np.cumsum(weighted_scores, axis=0)\n",
    "    \n",
    "    #get the weighted alpha\n",
    "    weighted_alpha = np.transpose(alpha) * (1/weights)\n",
    "    \n",
    "    #get elementwise product and sum each row\n",
    "    lambda_vals = lambda_0 + (weighted_alpha * cumul_sum_weighted_scores).sum(axis=1).reshape(-1,1)\n",
    "    \n",
    "    #return the lambda_values\n",
    "    return lambda_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neg_log_likelihood(params, lambda_0, T_vals, diagnostics=False):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "        params: ((k+1,)) parameters. First k are alpha and then delta\n",
    "        lambda_0: the baseline lambda\n",
    "        T_vals: ((N+1) x 1) event times\n",
    "    \n",
    "    OUTPUT:\n",
    "        value of the negative log likelihood function\n",
    "    \"\"\"\n",
    "    \n",
    "    #get alpha and delta and lambda_0\n",
    "    alpha = params[0]\n",
    "    delta = params[-1]\n",
    "    \n",
    "    #get weights\n",
    "    weights = np.exp(delta*T_vals)\n",
    "    \n",
    "    #get inverse weights\n",
    "    inv_weights = 1 / weights\n",
    "    \n",
    "    #get difference between inverse weights\n",
    "    diff_inv_weights = inv_weights[:-1] - inv_weights[1:]\n",
    "    \n",
    "    #get cumulative sum of weights\n",
    "    cumul_sum_weights_inc_first = np.cumsum(weights)[:-1].reshape(-1,1)\n",
    "    cumul_sum_weights_not_inc_first = np.cumsum(weights)[1:].reshape(-1,1)\n",
    "    \n",
    "    #calculate the sum term\n",
    "    sum_term = np.sum(np.log(lambda_0*np.ones_like(diff_inv_weights) + alpha*inv_weights[1:]*cumul_sum_weights_not_inc_first))\n",
    "    \n",
    "    #calculate the integral term \n",
    "    integral_term = lambda_0*T_vals[-1][0] + (alpha / delta) * np.sum(cumul_sum_weights_inc_first * diff_inv_weights)\n",
    "    \n",
    "    if diagnostics:\n",
    "        print('Sum Term: %s'%sum_term)\n",
    "        print('Integral Term: %s'%integral_term)\n",
    "        \n",
    "        print('inv_weights: \\n%s'%inv_weights[1:])\n",
    "        print('cumul_sum_weights_not_inc_first: \\n%s'%cumul_sum_weights_not_inc_first)\n",
    "        print('elementwise product: \\n%s'%(inv_weights[1:]*cumul_sum_weights_not_inc_first))\n",
    "        print('inside log: \\n%s'%(lambda_0 + alpha*inv_weights[1:]*cumul_sum_weights_not_inc_first))\n",
    "    \n",
    "    #return total negative log likelihood\n",
    "    return integral_term - sum_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_hawkes_process(params, lambda_0, T_vals, F, dt, T, viz=True, single_event=False):\n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "        params: ((k+1,)) parameters. First k are alpha then delta\n",
    "        lambda_0: the baseline lambda\n",
    "        F: (1 x k) first feature\n",
    "        T_vals: (1 x 1) first event time\n",
    "        dt: real number, step size during simulation\n",
    "        T: end time of simulation\n",
    "        lambda_0: the baseline event intensity\n",
    "        viz: boolean for whether we want to display a graph or not\n",
    "        \n",
    "        \n",
    "    OUTPUTS (in order):\n",
    "        T_vals: final times of all events\n",
    "        F: final features\n",
    "    \"\"\"\n",
    "    \n",
    "    #create list of lambda vals for viz\n",
    "    lambda_vals = []\n",
    "    \n",
    "    #create list of time vals for viz\n",
    "    time_vals = []\n",
    "    \n",
    "    #create list of event times for viz\n",
    "    event_times = []\n",
    "    \n",
    "    #get the current time as the first passed in event\n",
    "    curr_time = T_vals[-1][0] + dt\n",
    "    \n",
    "    #while we haven't yet reached out final time\n",
    "    while curr_time < T:\n",
    "        \n",
    "        #append this time val to a list for viz\n",
    "        time_vals.append(curr_time)\n",
    "        \n",
    "        #get the probability that we have an event right now\n",
    "        lambda_val = calculate_lambda_at_t_val(params, lambda_0, F, T_vals, curr_time)\n",
    "        prob = lambda_val * dt\n",
    "        \n",
    "        #append this lambda val to a list for viz\n",
    "        lambda_vals.append(lambda_val)\n",
    "        \n",
    "        #if we meet that probability, then add an event to the data\n",
    "        if random() < prob:\n",
    "            event_times.append(curr_time)\n",
    "            T_vals = np.concatenate([T_vals, [[curr_time]]], axis=0)\n",
    "            F = np.concatenate([F, [[1]]], axis=0)\n",
    "            if single_event:\n",
    "                return event_times[0]\n",
    "        \n",
    "        #increment current time\n",
    "        curr_time += dt\n",
    "        \n",
    "    #return right away if no viz\n",
    "    if not viz:\n",
    "        return T_vals, F\n",
    "    \n",
    "    #otherwise there is viz\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(time_vals, lambda_vals)\n",
    "    plt.xlabel('Time', fontsize=16)\n",
    "    plt.ylabel('lambda (event intensity)', fontsize=16)\n",
    "\n",
    "    for t in event_times:\n",
    "        plt.axvline(t, color='k', linestyle='--', alpha=.5)\n",
    "        \n",
    "    return T_vals, F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_params():\n",
    "    alpha = 1 - np.random.random()\n",
    "    delta = np.random.random()\n",
    "    return np.array([alpha, delta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recovered_params(T_vals, num_starts, true_params, lambda_0, dev_level=0.1):\n",
    "    \"\"\"\n",
    "    This function starts the minimization process from many random starting points and returns the recovered params\n",
    "    \"\"\"\n",
    "    \n",
    "    #set the constraints on the params\n",
    "    A_1 = np.array([[1,1], [0,1], [-1,0]])\n",
    "    constraint_1 = LinearConstraint(A_1, 0, np.inf)\n",
    "    \n",
    "    recovered_params = []\n",
    "    \n",
    "    for _ in range(num_starts):\n",
    "        \n",
    "        #get starting params\n",
    "        params_init = generate_random_params()\n",
    "\n",
    "        try:\n",
    "            #get recoverd params\n",
    "            result = minimize(calculate_neg_log_likelihood, x0=params_init, args=(lambda_0, T_vals), method='COBYLA', \\\n",
    "                              constraints=[{'type': 'ineq', 'fun': lambda x: -x[0]},\n",
    "                                           {'type': 'ineq', 'fun': lambda x: x[1]},\n",
    "                                           {'type': 'ineq', 'fun': lambda x: x[0] + x[1]}])\n",
    "            \n",
    "            if result.success == True:\n",
    "                recovered_params.append(result.x)\n",
    "        \n",
    "        except ValueError as v:\n",
    "            print(v)\n",
    "                \n",
    "    return np.array(recovered_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missing_param(true_params, lambda_0, T_vals, channel_name=''):\n",
    "    \n",
    "    t1_start = time()\n",
    "\n",
    "    for idx in range(true_params.shape[0]):\n",
    "        \n",
    "        var = 'alpha' if idx == 0 else 'delta'\n",
    "        \n",
    "        param_vals = np.arange(0,1,.01) if var=='delta' else np.arange(-1,0,.01)\n",
    "        \n",
    "        losses = np.empty_like(param_vals)\n",
    "        \n",
    "        min_pair = [-1,99999999]\n",
    "\n",
    "        for i in range(param_vals.shape[0]):\n",
    "            p_copy = true_params.copy()\n",
    "            p_copy[idx] = param_vals[i]\n",
    "            losses[i] = calculate_neg_log_likelihood(p_copy, lambda_0, T_vals)\n",
    "            if losses[i] < min_pair[1]:\n",
    "                min_pair[0] = param_vals[i]\n",
    "                min_pair[1] = losses[i]\n",
    "        \n",
    "        plt.plot(param_vals, losses)\n",
    "        plt.title('%s\\nTrue Value: %s\\nLoss Min Param: %s\\nLoss: %s'%(var, true_params[idx], round(min_pair[0],2), round(min_pair[1],2)), fontsize=16)\n",
    "        plt.xlabel(var, fontsize=14)\n",
    "        plt.ylabel('Loss', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig('%s_one_const_var=%s_lambda0=%s_alpha=%s_delta=%s.png'%(channel_name, var, lambda_0, true_params[0], true_params[1]))\n",
    "        plt.clf()\n",
    "        #print('Best %s: %s'%(var, round(min_pair[0],2)))\n",
    "        \n",
    "    t1_end = time()\n",
    "    #print('One-Param Time: %s'%(t1_end-t1_start))\n",
    "        \n",
    "    #whole grid search\n",
    "    \n",
    "    t2_start = time()\n",
    "\n",
    "    alpha_vals = np.arange(-1,0,.01)\n",
    "\n",
    "    delta_vals = np.arange(0,1,.01)\n",
    "\n",
    "    min_params = [-1,-1, 99999999]\n",
    "    \n",
    "    points = []\n",
    "\n",
    "    for alpha in alpha_vals:\n",
    "        for delta in delta_vals:\n",
    "            if delta + alpha > 0:\n",
    "                loss = calculate_neg_log_likelihood(np.array([alpha, delta]), lambda_0, T_vals)\n",
    "                if not np.isnan(loss):\n",
    "                    points.append((alpha, delta, loss))\n",
    "                if loss < min_params[-1]:\n",
    "                    min_params = [alpha, delta, loss]\n",
    "\n",
    "    print('Whole Grid Result: %s'%min_params)\n",
    "    \n",
    "    t2_end = time()\n",
    "    \n",
    "    plt.figure(figsize=(8,8))\n",
    "    losses = [i[2] for i in points]\n",
    "    min_loss = min(losses)\n",
    "    max_loss = max(losses)\n",
    "    #print('Min Loss: %s'%min_loss)\n",
    "    #print('Max Loss: %s'%max_loss)\n",
    "    losses = [(i - min_loss) / (max_loss - min_loss) for i in losses]\n",
    "    colors = [(l,l,l) for l in losses]\n",
    "    \n",
    "    plt.scatter([i[0] for i in points], [i[1] for i in points], color=colors, s=10)\n",
    "    plt.axvline(min_params[0], color='r', linestyle='--', alpha=0.75)\n",
    "    plt.axhline(min_params[1], color='r', linestyle='--', alpha=0.75)\n",
    "    \n",
    "    plt.title('Grid Search Results:\\nalpha=%s, delta=%s'%(round(min_params[0],2), round(min_params[1],2)), fontsize=18)\n",
    "    plt.xlabel('alpha', fontsize=16)\n",
    "    plt.ylabel('delta', fontsize=16)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('%s_grid_search_lambda_0=%s_alpha=%s_delta=%s.png'%(channel_name, lambda_0, true_params[0], true_params[1]))\n",
    "    \n",
    "    #print('Grid Search Time: %s'%(t2_end-t2_start))\n",
    "    \n",
    "    return min_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_event_times(T_vals, lambda_0, params, num_sims=1000):\n",
    "    next_event_times = []\n",
    "    F_init = np.ones_like(T_vals)\n",
    "    for _ in range(num_sims): \n",
    "        next_event_time = simulate_hawkes_process(params, lambda_0, T_vals, F_init, .01, 10000000, viz=False, single_event=True)\n",
    "        next_event_times.append(next_event_time)\n",
    "    return next_event_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_future_event(T_vals, limit, lambda_0, params, num_sims, channel_name):\n",
    "    num_included = len(T_vals[T_vals < limit]) + 1\n",
    "    next_event_times = get_next_event_times(T_vals[:num_included].reshape(-1,1), lambda_0, params, num_sims)\n",
    "    \n",
    "    avg = round(np.mean(next_event_times), 2)\n",
    "    dev = round(np.std(next_event_times), 2)\n",
    "    true = round(T_vals[num_included][0],2 )\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,4))\n",
    "    \n",
    "    plt.hist(next_event_times, color='cornflowerblue', edgecolor='black', linewidth=2, density=True)\n",
    "    plt.axvline(avg, alpha=0.75, linestyle='--', color='r')\n",
    "    plt.axvline(true, alpha=0.75, color='b')\n",
    "    plt.ylabel('Density', fontsize=16)\n",
    "    plt.xlabel('Predicted Next Event Time', fontsize=16)\n",
    "    plt.title('%s - After t=%s days\\nAvg. Pred=%s days\\nDev. Pred=%s days\\nTrue Event Time=%s days'%(channel_name, limit, avg, dev, true), fontsize=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig('%s_%sdays_prediction.png'%(channel_name, limit))\n",
    "    \"\"\"\n",
    "    return {'pred': avg, 'true': true, 'dev': dev}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_type_search(true_params, lambda_0, T_vals):\n",
    "    \n",
    "    #the alpha sub domain\n",
    "    #alpha_vals = np.arange(-1,0,.01)\n",
    "    alpha_vals = np.arange(0,.5,.005)\n",
    "\n",
    "    #the delta sub domain\n",
    "    delta_vals = np.arange(0,.5,.005)\n",
    "    \n",
    "    #get ogrid to enforce delta > -alpha\n",
    "    a,d = np.ogrid[0:len(alpha_vals), 0:len(delta_vals)]\n",
    "    \n",
    "    #precompute negative log likelihood values\n",
    "    NLL_vals = np.ones((len(alpha_vals), len(delta_vals)))*10000000000\n",
    "    \n",
    "    #print('start compute NLL')\n",
    "    for i,alpha in enumerate(alpha_vals):\n",
    "        for j,delta in enumerate(delta_vals):\n",
    "            if alpha < delta:\n",
    "                loss = calculate_neg_log_likelihood(np.array([alpha, delta]), lambda_0, T_vals)\n",
    "                if not np.isnan(loss):\n",
    "                    NLL_vals[i,j] = loss\n",
    "                    if loss < 0:\n",
    "                        print(alpha, delta, loss)\n",
    "                        print('-----')\n",
    "    #print('finish compute NLL')\n",
    "    \n",
    "    #initial (alpha_idx, delta_idx)\n",
    "    curr_params = [1,0]\n",
    "    while delta_vals[curr_params[1]] < alpha_vals[curr_params[0]]:\n",
    "        curr_params = [np.random.choice(range(len(alpha_vals))), np.random.choice(range(len(delta_vals)))]\n",
    "        \n",
    "    #print('Starting at %s'%[alpha_vals[curr_params[0]], delta_vals[curr_params[1]]])\n",
    "    curr_loss = NLL_vals[curr_params[0], curr_params[1]]\n",
    "    #print('Loss: %s'%curr_loss)\n",
    "    #print('=========')\n",
    "\n",
    "    loss_vals = [curr_loss]\n",
    "    param_vals = [curr_params]\n",
    "    \n",
    "    #set tolerance\n",
    "    tol = 1.01\n",
    "    \n",
    "    #iterate\n",
    "    max_iters = 100\n",
    "    curr_iter = 0\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    while curr_iter < max_iters:\n",
    "        \n",
    "        ##### CHOOSE NEXT DELTA #####\n",
    "        \n",
    "        alpha_index = curr_params[0]\n",
    "        \n",
    "        #get valid NLL_vals given this alpha\n",
    "        valid_indices = delta_vals > -alpha_vals[alpha_index]\n",
    "        valid_deltas = delta_vals[valid_indices]\n",
    "        valid_NLL_vals = NLL_vals[alpha_index][valid_indices]\n",
    "        valid_index_nums = np.nonzero(valid_indices)[0]\n",
    "        \n",
    "        #get indices of 10 lowest losses\n",
    "        mask = valid_NLL_vals < loss_vals[-1] * tol\n",
    "        \n",
    "        if mask.sum() <= 1:\n",
    "            break\n",
    "        \n",
    "        valid_NLL_vals = valid_NLL_vals[mask]\n",
    "        valid_index_nums = valid_index_nums[mask]\n",
    "        sorted_NLL_inds = valid_NLL_vals.argsort()\n",
    "        valid_NLL_vals = valid_NLL_vals[sorted_NLL_inds]\n",
    "        valid_index_nums = valid_index_nums[sorted_NLL_inds]\n",
    "        \n",
    "        #get valid weights and normalize\n",
    "        valid_weights = 1 / valid_NLL_vals\n",
    "        valid_weights = valid_weights / valid_weights.sum()\n",
    "        \n",
    "        chosen_delta_idx = np.random.choice(valid_index_nums, p=valid_weights)\n",
    "        curr_params[1] = chosen_delta_idx\n",
    "        #print('update delta: %s'%[alpha_vals[curr_params[0]], delta_vals[curr_params[1]]])\n",
    "        #print('Loss: %s'%NLL_vals[curr_params[0], curr_params[1]])\n",
    "        param_vals.append((alpha_vals[curr_params[0]], delta_vals[curr_params[1]]))\n",
    "        \n",
    "        ##### CHOOSE NEXT ALPHA #####\n",
    "        \n",
    "        delta_index = curr_params[1]\n",
    "        \n",
    "        #get valid NLL_vals given this delta\n",
    "        valid_indices = delta_vals[delta_index] > -alpha_vals\n",
    "        valid_alphas = alpha_vals[valid_indices]\n",
    "        valid_NLL_vals = NLL_vals[:,delta_index].flatten()[valid_indices]\n",
    "        valid_index_nums = np.nonzero(valid_indices)[0]\n",
    "        \n",
    "        #get indices of 10 lowest losses\n",
    "        mask = valid_NLL_vals < loss_vals[-1] * tol\n",
    "        \n",
    "        if mask.sum() <= 1:\n",
    "            break\n",
    "        \n",
    "        valid_NLL_vals = valid_NLL_vals[mask]\n",
    "        valid_index_nums = valid_index_nums[mask]\n",
    "        sorted_NLL_inds = valid_NLL_vals.argsort()[:10]\n",
    "        valid_NLL_vals = valid_NLL_vals[sorted_NLL_inds]\n",
    "        valid_index_nums = valid_index_nums[sorted_NLL_inds]\n",
    "        \n",
    "        #get valid weights and normalize\n",
    "        valid_weights = 1 / valid_NLL_vals\n",
    "        valid_weights = valid_weights / valid_weights.sum()\n",
    "        \n",
    "        chosen_alpha_idx = np.random.choice(valid_index_nums, p=valid_weights)\n",
    "        curr_params[0] = chosen_alpha_idx\n",
    "        #print('update alpha: %s'%[alpha_vals[curr_params[0]], delta_vals[curr_params[1]]])\n",
    "        #print('Loss: %s'%NLL_vals[curr_params[0], curr_params[1]])\n",
    "        \n",
    "        loss_vals.append(NLL_vals[curr_params[0], curr_params[1]])\n",
    "        param_vals.append((alpha_vals[curr_params[0]], delta_vals[curr_params[1]]))\n",
    "        \n",
    "        #print('=========')\n",
    "        \n",
    "        curr_iter += 1\n",
    "        \n",
    "    end = time()\n",
    "    #print('Total Time: %s'%((end-start)*1000))\n",
    "    \n",
    "    min_param_loc = np.where(NLL_vals == NLL_vals.min())\n",
    "    grid_min_alpha_idx = min_param_loc[0][0]\n",
    "    grid_min_delta_idx = min_param_loc[1][0]\n",
    "    grid_min_alpha = alpha_vals[grid_min_alpha_idx]\n",
    "    grid_min_delta = delta_vals[grid_min_delta_idx]\n",
    "    \n",
    "    return param_vals[-1]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(loss_vals[1:])\n",
    "    plt.xlabel('Step', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    #plt.title('Gibbs-Type Search\\nTrue Params: lambda_0=%s, alpha=%s, delta=%s'%(lambda_0, true_params[0], true_params[1]), fontsize=18)\n",
    "    #print('Grid Min Loss:', NLL_vals.min())\n",
    "    #print('Gibbs Min Loss:', loss_vals[-1])\n",
    "    \n",
    "    #print('Grid Solution:', (grid_min_alpha, grid_min_delta))\n",
    "    #print('Gibbs Solution:', (param_vals[-1][0], param_vals[-1][1]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('Gibbs_Type Search_True Params_lambda_0=%s_alpha=%s_delta=%s.png'%(lambda_0, true_params[0], true_params[1]))\n",
    "    \n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    \n",
    "    min_alpha = min([item[0] for item in param_vals]) - .1\n",
    "    max_alpha = max([item[0] for item in param_vals]) + .1\n",
    "    \n",
    "    min_delta = min([item[1] for item in param_vals]) - .1\n",
    "    max_delta = max([item[1] for item in param_vals]) + .1\n",
    "    \n",
    "    for i in range(len(param_vals)-1):\n",
    "        alpha_last, delta_last = param_vals[i]\n",
    "        alpha_next, delta_next = param_vals[i+1]\n",
    "        plt.plot((alpha_last, alpha_next), (delta_last, delta_next), marker = 'o', color='k')\n",
    "        \n",
    "    search_start_loc = plt.plot((param_vals[0][0], param_vals[0][0]), (param_vals[0][1], param_vals[0][1]), marker = 'o', color='r', linewidth=.5)\n",
    "    true_loc = plt.plot((true_params[0], true_params[0]), (true_params[1], true_params[1]), marker = '*', color='orange', markersize=10)\n",
    "    grid_search_loc = plt.plot((grid_min_alpha, grid_min_alpha), (grid_min_delta, grid_min_delta), marker = 'x', color='magenta', markersize=10)\n",
    "        \n",
    "    plt.xlim(min_alpha, max_alpha)\n",
    "    plt.ylim(min_delta, max_delta)\n",
    "    plt.xlabel('alpha', fontsize=16)\n",
    "    plt.ylabel('delta', fontsize=16)\n",
    "    plt.title('Gibbs Sampler Path\\nTrue Params:\\nalpha=%s, delta=%s'%(true_params[0], true_params[1]), fontsize=18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Gibbs_Sampler_Path_True Params_lambda=%s_alpha=%s_delta=%s.png'%(lambda_0, true_params[0], true_params[1]))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gibbs_sols = {}\n",
    "for name, data in channel_name_to_events_list.items():\n",
    "    print(name)\n",
    "    trial_sols = []\n",
    "    for _ in range(10):\n",
    "        trial_sols.append(gibbs_type_search(None, 0.1, data['series']))\n",
    "    gibbs_sols[name] = [np.mean([item[i] for item in trial_sols]) for i in range(2)]\n",
    "    print(gibbs_sols[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel_name_to_preds = {}\n",
    "for channel_name in channel_name_to_events_list:\n",
    "    for time_lim in [20,40]:\n",
    "        print(channel_name, time_lim)\n",
    "        result = simulate_future_event(channel_name_to_events_list[channel_name]['series'], time_lim, 0.1, np.array(gibbs_sols[channel_name]), 1000, channel_name)\n",
    "        channel_name_to_preds[(channel_name, time_lim)] = result\n",
    "        print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = []\n",
    "avg_time_between_uploads = []\n",
    "num_uploads = []\n",
    "pct_error_20 = []\n",
    "pct_error_40 = []\n",
    "estimated_alpha = []\n",
    "estimated_K = []\n",
    "areas = []\n",
    "\n",
    "for channel_name in channel_name_to_events_list:\n",
    "    channel_names.append(channel_name)\n",
    "    areas.append(channel_name_to_events_list[channel_name]['area'])\n",
    "    series = channel_name_to_events_list[channel_name]['series']\n",
    "    avg_time_between_uploads.append(np.diff(series.flatten()).mean())\n",
    "    num_uploads.append(len(series))\n",
    "    sols = gibbs_sols[channel_name]\n",
    "    estimated_alpha.append(sols[1])\n",
    "    estimated_K.append(sols[0] / sols[1])\n",
    "    pct_error_20.append(abs(channel_name_to_preds[(channel_name, 20)]['pred'] - channel_name_to_preds[(channel_name, 20)]['true']) / channel_name_to_preds[(channel_name, 20)]['true'])\n",
    "    pct_error_40.append(abs(channel_name_to_preds[(channel_name, 40)]['pred'] - channel_name_to_preds[(channel_name, 40)]['true']) / channel_name_to_preds[(channel_name, 40)]['true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame(columns=['channel_name', 'area', 'avg_days_between_uploads', 'num_uploads', 'estimated_alpha', 'estimated_K', 'pct_error_20', 'pct_error_40'], data=zip(channel_names, areas, avg_time_between_uploads, num_uploads, estimated_alpha, estimated_K, pct_error_20, pct_error_40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between percent error 20 and avg days between uploads\n",
    "pearsonr(df_preds.avg_days_between_uploads, df_preds.pct_error_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between percent error 40 and avg days between uploads\n",
    "pearsonr(df_preds.avg_days_between_uploads, df_preds.pct_error_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between alpha and avg days between uploads\n",
    "pearsonr(df_preds.estimated_alpha, df_preds.avg_days_between_uploads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between K and avg days between uploads\n",
    "pearsonr(df_preds.estimated_K, df_preds.avg_days_between_uploads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between pct error 20 and estimated alpha\n",
    "pearsonr(df_preds.estimated_alpha, df_preds.pct_error_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between pct error 40 and estimated alpha\n",
    "pearsonr(df_preds.estimated_alpha, df_preds.pct_error_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between pct error 20 and estimated K\n",
    "pearsonr(df_preds.estimated_K, df_preds.pct_error_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation between pct error 40 and estimated K\n",
    "pearsonr(df_preds.estimated_K, df_preds.pct_error_40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.groupby('area').mean().sort_values('pct_error_20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = np.zeros((len(df_preds.columns[2:]), len(df_preds.columns[2:])))\n",
    "significant_matrix = np.zeros((len(df_preds.columns[2:]), len(df_preds.columns[2:]))).astype(bool)\n",
    "for i1,c1 in enumerate(df_preds.columns[2:]):\n",
    "    for i2,c2 in enumerate(df_preds.columns[2:]):\n",
    "        res = pearsonr(df_preds[c1], df_preds[c2])\n",
    "        correlation_matrix[i1,i2] = res[0]\n",
    "        if res[1] < 0.05:\n",
    "            significant_matrix[i1,i2] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set figure size\n",
    "plt.figure(figsize=(10,8))\n",
    "#Specify we would like a heatmap\n",
    "plt.imshow(correlation_matrix, interpolation = 'nearest', cmap = 'Greys')\n",
    "#Specify the x and y labels\n",
    "plt.xticks(range(6), df_preds.columns[2:], rotation = 90, fontsize = 16)\n",
    "plt.yticks(range(6), df_preds.columns[2:], fontsize = 16)\n",
    "\n",
    "#include a colorbar for reference\n",
    "plt.ylim(-.5,5.5)\n",
    "plt.xlim(-.5,5.5)\n",
    "plt.colorbar()\n",
    "\n",
    "for i1 in range(6):\n",
    "    for i2 in range(6):\n",
    "        if significant_matrix[i1,i2]:\n",
    "            plt.scatter([i1],[i2], marker='x', color='r')\n",
    "            \n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_heatmap.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T_vals_init = np.array([0]).reshape((1,1))\n",
    "F_init = np.ones((1,1))\n",
    "\n",
    "lambda_0 = 0.1\n",
    "alpha = 0.48\n",
    "K = .4/.48\n",
    "\n",
    "true_params = np.array([alpha*K, alpha])\n",
    "T_vals_final, F_final = simulate_hawkes_process(np.array(gibbs_sols[channel_name]), lambda_0, T_vals_init, F_init, .01, 93, viz=True)\n",
    "\n",
    "gaps = np.diff(T_vals_final.reshape(1,-1))\n",
    "\n",
    "print('Avg Gap:', np.mean(gaps))\n",
    "print('Dev Gap:', np.std(gaps))\n",
    "print('Num Events:', len(T_vals_final))\n",
    "\n",
    "plt.title('Simulated Hawkes Process\\nlambda_0=%s, alpha=%s, K=%s'%(lambda_0, alpha, K), fontsize=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('updated_hawkes_process_sim_lambda_0=%s_alpha=%s_K=%s.png'%(lambda_0, alpha, K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_name_to_events_list = pickle.load( open( \"event_times.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, data in channel_name_to_events_list.items():\n",
    "    channel_name_to_events_list[name]['series'] = np.array(channel_name_to_events_list[name]['series']).reshape((-1,1)) / 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try to recover params - COBYLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_0 = 1\n",
    "\n",
    "num_trials = 1000\n",
    "\n",
    "start = time()\n",
    "result = get_recovered_params(allrecipes_T_vals, num_trials, true_params, lambda_0, .5)\n",
    "end = time()\n",
    "print('Avg Milliseconds: %s'%str((end - start)/num_trials*1000))\n",
    "\n",
    "print('Success Rate:', len(result) / num_trials)\n",
    "\n",
    "print('Avg Recoverd Params:', result.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search and One-Held Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_0 = 1\n",
    "true_params = np.array([-0.15, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = get_missing_param(true_params, lambda_0, allrecipes_T_vals, 'allrecipes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs-Type Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_vals = gibbs_type_search(None, 0.1, channel_name_to_events_list['Khan Academy']['series'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate with Recovered Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_vals_init = np.array([0]).reshape((1,1))\n",
    "F_init = np.ones((1,1))\n",
    "recovered_params = np.array([-.02, .05])\n",
    "T_vals_final, F_final = simulate_hawkes_process(recovered_params, lambda_0, T_vals_init, F_init, .01, allrecipes_T_vals[-1][0], viz=True)\n",
    "\n",
    "gaps = np.diff(T_vals_final.reshape(1,-1))\n",
    "\n",
    "avg_gap = round(np.mean(gaps), 2)\n",
    "dev_gap = round(np.std(gaps), 2)\n",
    "num_events = len(T_vals_final)\n",
    "\n",
    "print('Avg Gap:', avg_gap)\n",
    "print('Dev Gap:', dev_gap)\n",
    "print('Num Events:', num_events)\n",
    "plt.title('Hawkes Process from Recovered Params\\nYouTube Channel: Allrecipes\\nAvg. Gap: %s | Dev. Gap: %s | Num Events: %s'%(avg_gap, dev_gap, num_events), fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
